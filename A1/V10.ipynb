{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Processamento de Linguagem Natural (NLP)\n",
    "\n",
    "Professor: Arlindo Galvão\n",
    "\n",
    "Data: 03/09/2024\n",
    "\n",
    "## Cronograma\n",
    "\n",
    "* Parte I: N-Gram Language Model\n",
    "\n",
    "* Parte II: Character-Level RNN Language Model\n",
    "\n",
    "## OBS\n",
    "Deixar registrado as repostas nas saídas das celulas do notebook de submissão."
   ],
   "id": "f63653ae1c8a6254"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parte I: N-Gram Language Model",
   "id": "800836ee52281d07"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:01:34.982121Z",
     "start_time": "2024-09-23T22:01:32.394647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''Imports'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import unicodedata\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.corpus import stopwords"
   ],
   "id": "73e512050df3ea62",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:29:01.829081Z",
     "start_time": "2024-09-20T01:29:01.816962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''Classe N-gram\n",
    "    O modelo N-Gram consiste em utilizar os n últimos tokens para prever o próximo token, realizando uma espécie de janela deslizante no texto.\n",
    "    O modelo recebe um token em sua representação numérica e utiliza embeddings para converter essa representação em vetores densos situados em um espaço vetorial.\n",
    "    Esses vetores são passados para uma camada linear que os processa e gera uma saída de tamanho 128, que será passada na rede para gerar a distribuição de scores sobre cada token do vocabulário. (utilizando a LogSoftmax)\n",
    "    \n",
    "    Vale ressaltar que esse modelo possui entrada fixa, como também não faz um processamento temporal da entrada.\n",
    "'''\n",
    "\n",
    "class NGramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim) # Gera uma matriz com vocab_size linhas e embedding_dim colunas, onde cada token é associado a um índice e nesse indíce está os embeddings associados aquele token, no inicio, aleatórios\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)   # FC com 128 neurônios, casa um com um peso para cada dimensão de cada token de entrada\n",
    "        self.linear2 = nn.Linear(128, vocab_size) # FC com vocab_size neurônios, pois cada neurônio é responsável por gerar uma probabilidade para cada token do vocabulário\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1)) # Gera embeddings para os tokens de entrada e achata eles para passar para a camada linear\n",
    "        out = F.relu(self.linear1(embeds)) # Aplica a camada linear e a função de ativação ReLU\n",
    "        out = self.linear2(out) # Aplica a segunda camada linear, reponsável por gerar as probabilidades\n",
    "        log_probs = F.log_softmax(out, dim=1) # Aplica a função de ativação softmax para gerar uma distribuição de probabilidade\n",
    "        return log_probs"
   ],
   "id": "779a2be6afe46318",
   "execution_count": 1262,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:29:02.044173Z",
     "start_time": "2024-09-20T01:29:02.018376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parâmetros do modelo\n",
    "'''O context_size define quantos grams passaremos para o modelo, ou seja, quantos tokens passados serão utilizados para prever o próximo token.\n",
    "Já o embedding_dim define quantas posições o vetor de representação das palavras possuirá, assim, a palavra/token passa a ser representada como um vetor com embedding_dim dimensões.'''\n",
    "\n",
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()"
   ],
   "id": "220b8db069eba831",
   "execution_count": 1263,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:29:02.219753Z",
     "start_time": "2024-09-20T01:29:02.216005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Criar n-grams\n",
    "ngrams = [([test_sentence[i - CONTEXT_SIZE + j] for j in range(CONTEXT_SIZE)], test_sentence[i]) for i in range(CONTEXT_SIZE, len(test_sentence))]\n",
    "\n",
    "# Construir o vocabulário\n",
    "vocab = set(test_sentence) # A priori, cada token está sendo considerado como uma palavra\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)} # Cada palavra recebe seu índice, e esses índices servem para localizar os embeddings desa palavra\n",
    "ix_to_word = {i: word for word, i in word_to_ix.items()}"
   ],
   "id": "70928d6ec184e593",
   "execution_count": 1264,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:29:03.772757Z",
     "start_time": "2024-09-20T01:29:03.621306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Treinamento\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams: # Para cada n-grama \n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_idxs)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)\n",
    "\n",
    "    print(f'{epoch} - Loss: {total_loss}')\n",
    "\n",
    "\n",
    "#print(\"Losses:\", losses)"
   ],
   "id": "41bb366dd7cdfef1",
   "execution_count": 1265,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Atividades\n",
    "\n",
    "__1 - Escreva a função generate da classe NGramLanguageModel.__\n",
    "\n",
    "__2 - Depois de treinar o modelo, gere uma sentença de 128 tokens.__\n",
    "\n",
    "__3 - Calcule e print a similaridade entre duas palavras. A similaridade resultante está correta? Justifique a sua resposta.__\n",
    "\n",
    "__4 - Proponha três alterações no código e demonstre que melhorou o desempenho do modelo.__"
   ],
   "id": "7aa7d8ebd9ea02c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "####  Atividade 1",
   "id": "8bc3e375d8bfe736"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "REQUISITOS:\n",
    "Para gerar 128 tokens é necessário:\n",
    "- ter um contexto para passar para o modelo\n",
    "- realizar o foward no modelo já treinado com esse contexto inicial\n",
    "- escolher o token com maior probabilidade e adicionar ao contexto\n",
    "- pegar os ultimos n grams do contexto e passar para o modelo (etapa 1)\n"
   ],
   "id": "3a25f70c06f7996d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:29:04.472294Z",
     "start_time": "2024-09-20T01:29:04.466989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''O código da função generate segue a seguinte lógica:\n",
    "    - Recebe o modelo, o contexto inicial (necessário para a primeira predição) e o número de palavras/tokens a serem gerados\n",
    "    \n",
    "    - Para cada palavra que deseja gerar:\n",
    "        - Transforma o contexto em índices para acessar seus respectivos embeddings\n",
    "        - Com o modelo treinado, retorna as probabilidades de cada token/palavra do vocabulário\n",
    "        - Escolhe a palavra de maior probabilidade\n",
    "        - Adiciona a nova palavra prevista ao contexto\n",
    "        - Retira o primeiro elemento do contexto para dar continuidade ao texto com o mesmo tamanho de entrada\n",
    "        - Printa a palavra prevista\n",
    "'''\n",
    "\n",
    "def generate(model, contexto, n_palavras):\n",
    "    for i in range(n_palavras): # Para cada palavra que queremos gerar\n",
    "        context_idxs = torch.tensor([word_to_ix[w] for w in contexto], dtype=torch.long) # Transforma o contexto em índices para acessar seus respectivos embeddings\n",
    "\n",
    "        with torch.no_grad(): # Utilizando torch.no_grad() para não armazenar os gradientes \n",
    "            log_probs = model(context_idxs) # Retorna as probabilidades de cada token/palavra do vocabulário, uma estratégia poderia ser fazer um top-k, onde pega-se as k palavras com maior probabilidade e escolhe-se uma aleatória, dando uma estocasticidade ao modelo.\n",
    "\n",
    "        palavra_predita_id = torch.argmax(log_probs).item() # Escolhe-se a palavra de maior probabilidade\n",
    "        palavra_predita = ix_to_word[palavra_predita_id] # Acessa-se a palavra associada ao índice\n",
    "\n",
    "        contexto.pop(0) # Retira o primeiro elemento do contexto para adicionar a nova palavra prevista e dar continuidade ao texto\n",
    "        contexto.append(palavra_predita)\n",
    "\n",
    "        print(palavra_predita, end=' ')\n"
   ],
   "id": "3765a12d5116cc57",
   "execution_count": 1266,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:29:04.911804Z",
     "start_time": "2024-09-20T01:29:04.903121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "contexto = ['When', 'forty']\n",
    "n_palavras = 128\n",
    "\n",
    "generate(model, contexto, n_palavras)"
   ],
   "id": "cfc11e1f42e98e4",
   "execution_count": 1267,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Atividade 3",
   "id": "a01e587b5139869c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Para calcular a similaridade entre duas palavras, será utilizado similaridade de cossenos. Com isso, enxergamos os tokens(palavras) como vetores em um espaço vetorial com a hipótese de que as distâncias preservam informação semântica (não necessariamente verdade), visto que utilizamos palavras vizinhas para prever a próxima palavra, assim, a representação (embedding) leva em consideração a semântica, visto que duas palavras que aparexem em contextos similares tendem a ter representações similares.\n",
    "\n",
    "Para isso, precisamos:\n",
    "- Escolher duas palavras\n",
    "- Obter os embeddings das palavras (representação vetorial)\n",
    "- Calcular a similaridade de cossenos entre os vetores através da formula:\n",
    "- $cos(\\theta) = \\frac{A \\cdot B}{||A|| \\cdot ||B||}$\n",
    "\n",
    "A similaridade de cossenos nos retorna um valor entre -1 e 1, onde 1 indica que os vetores \"apontam\" na mesma direção dentro do espaço e -1 indica que são opostos e 0 indica que são ortogonais.\n",
    "\n",
    "A medida de similaridade de cossenos nos traz a informação sobre a direção do vetor dentro do espaço, ignorando a sua magnitude, por isso, é possível que dois vetores tenham similaridade de cossenos alta, mas distâncias euclidianas diferentes."
   ],
   "id": "822d6b13f43ab04a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:29:05.712772Z",
     "start_time": "2024-09-20T01:29:05.707573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def similaridade_cossenos(model, palavra1, palavra2):\n",
    "    idx_palavra1 = torch.tensor([word_to_ix[palavra1]], dtype=torch.long) # Transforma a palavra em seu índice para acessar o embedding (representação vetorial)\n",
    "    idx_palavra2 = torch.tensor([word_to_ix[palavra2]], dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        v1 = model.embeddings(idx_palavra1) # Acessa o embedding da palavra, trazendo sua representação vetorial\n",
    "        v2 = model.embeddings(idx_palavra2)\n",
    "\n",
    "    produto_interno = torch.dot(v1.view(-1), v2.view(-1)) # Realiza o produto interno entre os vetores, que pode ser visto como uma distância entre os vetores no espaço\n",
    "\n",
    "    norma_v1 = torch.norm(v1) # Calcula a norma dos vetores\n",
    "    norma_v2 = torch.norm(v2)\n",
    "\n",
    "    similaridade = produto_interno / (norma_v1 * norma_v2)\n",
    "\n",
    "\n",
    "    return round(similaridade.item(), 4) # Retorna a similaridade arrendondada em 4 casas decimais"
   ],
   "id": "e03564aabeff7745",
   "execution_count": 1268,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:29:05.974984Z",
     "start_time": "2024-09-20T01:29:05.970581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "palavras_teste = [['beauty', 'beauty'], ['beauty', 'field,'], ['beauty', 'shame,'], ['beauty', 'praise'], ['beauty', \"beauty's\"]]\n",
    "\n",
    "for p1, p2 in palavras_teste:\n",
    "    print(f'Similaridade entre {p1} e {p2}: {similaridade_cossenos(model, p1, p2)}')"
   ],
   "id": "849524ca4098f512",
   "execution_count": 1269,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vale ressaltar que não necessariamente essas representações vetoriais preservam informação semântica ou alguma \"noção\" sobre as palavras, isso por que o modelo é puramente matemático, ná prática, a representação de uma palavra vai ser corrigida para minimizar uma função (loss) para aquela tarefa específica, e não necessariamente para preservar informação semântica ou fazer algum sentido para o ser humano. Ainda assim, assumimos que a têndencia é, que se treinado para uma tarefa em que dado uma palavra utilizamos seu contexto, e vice versa, provavelmente teremos uma informação semântica preservada, isso por que, se o corpus for suficientemente grande, palavras \"similares\" tenderão a aparecer em contextos similares, com isso, é como se repetidas vezes, ao entrar com a mesma (ou quase a mesma) informação na rede tivessemos duas ou mais palavras altamente provaveis, com isso, o natural é que ambas tenham representações vetoriais parecidas. No texto acima, o corpus não é suficientemente grande para que isso aconteça.",
   "id": "e4dffde9e45f34d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Atividade 4",
   "id": "a743560c56a45b74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Alteração 1:** Aumentar a quantidade de ajustes do modelo. O modelo incialmente proposto realiza uma atualização de pesos a cada amostra passada, porém faz isso apenas 10 vezes. Como temos poucos dados, na prática temos também poucos ajustes, aumentar a quantidade de ajustes pode melhorar o desempenho do modelo.\n",
    "\n",
    "**Alteração 2:** Aumentar a dimensão dos embeddings. Aumentar a dimensão dos embeddings pode ser uma estratégia para melhorar a representação semântica das palavras.\n",
    "\n",
    "**Alteração 3:** Aumentar o contexto. Aumentar o contexto pode ser uma estratégia para melhorar a previsão da próxima palavra, visto que mais palavras podem ser utilizadas para prever a próxima palavra.\n",
    "\n",
    "**Alteração 4:** Aumentar learning_rate"
   ],
   "id": "53186403b28d760b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:29:27.776491Z",
     "start_time": "2024-09-20T01:29:06.908784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_experimentos = 4\n",
    "resultados = []\n",
    "contexto = ['When', 'forty']\n",
    "n_palavras = 128\n",
    "\n",
    "for experimento in range(num_experimentos):\n",
    "\n",
    "    if experimento == 0:\n",
    "        print(\"Experimento 1: Aumentar a quantidade de ajustes do modelo\")\n",
    "        epocas = 300\n",
    "        print_every = 50\n",
    "        learning_rate = 0.001\n",
    "        CONTEXT_SIZE = 2\n",
    "        EMBEDDING_DIM = 10\n",
    "\n",
    "        ngrams = [([test_sentence[i - CONTEXT_SIZE + j] for j in range(CONTEXT_SIZE)], test_sentence[i]) for i in range(CONTEXT_SIZE, len(test_sentence))]\n",
    "\n",
    "        # Construir o vocabulário\n",
    "        vocab = set(test_sentence)\n",
    "        word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "        ix_to_word = {i: word for word, i in word_to_ix.items()}\n",
    "\n",
    "    if experimento == 1:\n",
    "        print(\"Experimento 2: Aumentar a dimensão dos embeddings\")\n",
    "        EMBEDDING_DIM = 30\n",
    "\n",
    "    if experimento == 2:\n",
    "        print(\"Experimento 3: Aumentar o contexto\")\n",
    "        CONTEXT_SIZE = 4\n",
    "        contexto = ['When', 'forty', 'winters', 'shall' ]\n",
    "        ngrams = [([test_sentence[i - CONTEXT_SIZE + j] for j in range(CONTEXT_SIZE)], test_sentence[i]) for i in range(CONTEXT_SIZE, len(test_sentence))]\n",
    "\n",
    "    if experimento == 3:\n",
    "        print(\"Experimento 4: Aumentar learning_rate\")\n",
    "        learning_rate = 0.01\n",
    "\n",
    "    print(f\"Experimento {experimento + 1} de {num_experimentos}\")\n",
    "\n",
    "    model = NGramLanguageModel(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.NLLLoss()\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epocas):\n",
    "        total_loss = 0\n",
    "        for context, target in ngrams:\n",
    "            context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "            model.zero_grad()\n",
    "            log_probs = model(context_idxs)\n",
    "            loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype=torch.long))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(f'Época: {epoch} - Loss: {total_loss}')\n",
    "\n",
    "    resultados.append(losses)\n",
    "    print()\n",
    "\n",
    "    # Gerando texto\n",
    "    print('Gerando texto: ', end=' ')\n",
    "    generate(model, contexto, n_palavras)  # Corrigido passando word_to_ix e ix_to_word\n",
    "    print('\\n')\n",
    "\n",
    "# Exibir os resultados ao final de todos os experimentos\n",
    "for i, resultado in enumerate(resultados):\n",
    "    print(f\"Resultado do experimento {i + 1}: Última perda: {resultado[-1]}\")\n"
   ],
   "id": "ef337bf662f5c972",
   "execution_count": 1270,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:29:27.946258Z",
     "start_time": "2024-09-20T01:29:27.779310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"PLOTAR GRAFICO COMPARANDO OS 4 MODELOS E LOSS AO LONGO DO TEMPO\"\"\"\n",
    "\n",
    "def plotar_perdas(resultados):\n",
    "    \"\"\"\n",
    "    Plota as curvas de perda para cada experimento.\n",
    "\n",
    "    Parâmetros:\n",
    "    resultados (list of lists): Lista onde cada sublista contém as perdas por época de um experimento.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))  # Define o tamanho da figura\n",
    "\n",
    "    # Define um ciclo de cores para os experimentos\n",
    "    cores = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'orange', 'purple', 'brown']\n",
    "\n",
    "    for i, perdas in enumerate(resultados):\n",
    "        epocas = range(1, len(perdas) + 1)\n",
    "        cor = cores[i % len(cores)]  # Cicla as cores se houver mais experimentos que cores disponíveis\n",
    "        plt.plot(epocas, perdas, label=f'Experimento {i+1}', color=cor)\n",
    "\n",
    "    plt.xlabel('Épocas', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.title('Curvas de Perda por Experimento', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotar_perdas(resultados)"
   ],
   "id": "2d5320ff081fb34",
   "execution_count": 1271,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Parte II: Character-Level RNN Language Model",
   "id": "ce13050339a5f474"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:01:36.575975Z",
     "start_time": "2024-09-23T22:01:36.571701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"As RNN lidam com sequências de dados, de maneira que cada item da sequencia é processado em um instante de tempo. Com isso, fazemos o processamento sequencial da entrada. O papel da RNN consiste em gerar uma boa representação da sequência em um vetor que chamamos de hidden_state, de maneira que o hidden_state é o vetor que em teoria armazena informação de todos os itens da sequência. Esse hidden_state é atualizado a cada novo item, permitindo que a rede aprenda dependências temporais. Na prática, temos duas redes dentro do bloco da rnn, uma responsável por receber o hidden_state atual e o item da sequência naquele instante de tempo e gerar uma nova representação desse hidden_state, já o outro modelo recebe esse hidden_state e realiza uma classificação, assim, é possível avaliar a qualidade da representação do hidden gerado pela primeira rede. \n",
    "\n",
    "Temos também que o processamento de sequências pode ser um problema, se lembrarmos das redes neurais, vemos que a partir de um certo número de camadas, começamos a enfrentar problemas com o gradiente, que pode explodir ou sumir (exploding e vanish gradient). Isso motivou o surgimento de técnicas como o batch_normalization e Ligações residuais. No processamento de sequências esse problema está ainda mais presente do que nas redes vanillas, isso por que, além de que quanto mais itens na sequência mais dependencia de camadas temos e mais fácil de termos problemas com o gradiente, ainda por que, estamos desenrolando a mesma rede ao longo do tempo, com isso, ainda existe a desvantagem das redes recorrentes de possuir um grau de liberdade a menos que as redes feedforwar.\"\"\"\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input_combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        output = self.i2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ],
   "id": "16b3b1311c445025",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Funções auxiliares (Questão 2)",
   "id": "bb6694faeb76b75b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:49:52.195628Z",
     "start_time": "2024-09-24T00:49:52.187050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''FUNÇÃO LOAD_DATASET'''\n",
    "def load_dataset(caminho_arquivo):\n",
    "    with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "'''FUNÇÃO PREPROCESS_DATA'''\n",
    "def preprocess_data(corpus):\n",
    "    corpus = unicodedata.normalize('NFD', corpus)  # Remove acentuação\n",
    "    corpus = re.sub(r'\\s+', ' ', corpus)  # Substitui quebras de linha e espaços múltiplos por um único espaço\n",
    "    corpus = corpus.lower()  # Converte para minúsculas\n",
    "    corpus = re.sub(r'[^a-zA-Z0-9.,;:!?\\'\" -]', '', corpus)  # Remove tudo exceto letras, números, pontuações e espaços\n",
    "\n",
    "    corpus = corpus.replace('--', '')\n",
    "    return corpus\n",
    "\n",
    "\n",
    "'''FUNÇÃO TOKENIZE_DATA'''\n",
    "def tokenize_data(corpus):\n",
    "    caracteres = list(corpus) #quebrando o texto em uma lista onde cada elemento é um caracter em ordem\n",
    "\n",
    "    vocabulario = {caractere: i for i, caractere in enumerate(set(caracteres))} \n",
    "    tokens = [vocabulario[caractere] for caractere in caracteres]\n",
    "\n",
    "    return tokens, vocabulario"
   ],
   "id": "fe233008a61301bf",
   "execution_count": 270,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:01:54.221180Z",
     "start_time": "2024-09-23T22:01:53.868798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_sentence = load_dataset('obama-data.txt')\n",
    "\n",
    "test_sentence = preprocess_data(test_sentence)\n",
    "test_sentence, vocab = tokenize_data(test_sentence)\n",
    "\n",
    "vocab_len = len(vocab)\n",
    "vocab_ix_to_char = {v: k for k, v in vocab.items()}"
   ],
   "id": "e573f38996eb583c",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:49:54.504587Z",
     "start_time": "2024-09-24T00:49:54.501529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''Para a função create_input, recebemos os tokens (uma lista de números que representam caracteres), com isso, geramos um tensor de entrada que tem tamanho max_sequence_len e cada item dentro do tensor é um token representado por um one-hot-encoding. O target é um tensor do mesmo tamanho do input, onde a representação dos tokens se dá de forma númerica, pois serão vistos como classes na hora do treinamento, além disso, o target_tensor é simplesmente o tensor de input com um shift para direita, pois para cada token + hidden_state que analizarmos o target é o próximo token.'''\n",
    "\n",
    "def create_input(tokens, start_idx, max_sequence_len):\n",
    "    input = []\n",
    "\n",
    "    t = torch.tensor(tokens[start_idx: start_idx + max_sequence_len], dtype=torch.long)\n",
    "\n",
    "    for i in range(max_sequence_len):\n",
    "        one_hot = torch.zeros(1, vocab_len)  # Inicializando o input do mesmo formato do hidden, para facilitar a concatenação dentro do modelo\n",
    "        one_hot[0][t[i]] = 1  # Preenche 1 no indice do token passado (representação do token em one-hot)\n",
    "        input.append(one_hot)\n",
    "\n",
    "    target = torch.tensor(tokens[start_idx + 1: start_idx + max_sequence_len + 1], dtype=torch.long) # Cria o tensor de target, onde o mesmo é o próximo input sem a representação vetorizada\n",
    "\n",
    "    input = torch.stack(input) # A sequencia é uma pilha com max_sequence_len tensores dentro, cada tensor é a representação daquele token \n",
    "\n",
    "    return input, target"
   ],
   "id": "7a7a76f44a6b3bf2",
   "execution_count": 271,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:01:54.231602Z",
     "start_time": "2024-09-23T22:01:54.228210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parâmetros do modelo\n",
    "n_hidden = 64\n",
    "learning_rate = 0.001\n",
    "n_epochs = 5\n",
    "print_every = 1\n",
    "max_sequence_len = 25"
   ],
   "id": "e311638bcababcc6",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:01:57.360832Z",
     "start_time": "2024-09-23T22:01:57.345439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rnn = RNN(vocab_len, n_hidden, vocab_len)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "rnn"
   ],
   "id": "c8ff319c1cc5471c",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Primeira coisa que temos definido é que existe uma RNN que trabalha a nível de caracter, ou seja, cada caracter deve virar um token, nesse sentido, temos um vocabulário de tamanho 46. Desse modo, o tamanho da entrada é o tamanho do vocabulário + o tamanho di hidden state, isso por que cada token é representado no formato de one hot encode, onde na posição referente ao seu valor númerico como token, temos o valor 1, e 0 no restante, assim, cada token tem em sua representação o tamanho do vocabulário. \n",
    "\n",
    "Além disso, a rnn por si só pode ser entendido como um modelo que, dada uma sequência, gera a melhor representação possível daquela sequência em um vetor de tamanho hidden_size, onde esse vetor de hidden_size será usado para classificação, no nosso caso, do próximo token. Por isso existem duas totalmente conectadas, uma responsável por gerar a melhor representação possível, e outra por gerar a probabilidde de cada token do vocabulário ser o próximo token a partir dessa representação. O aprendizado dessa rede funciona por BTT, que possui alguns problemas conforme as sequências crescem, como o gradiente explodir ou desaparecer."
   ],
   "id": "ef3acb7f755bdae1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:52:22.844452Z",
     "start_time": "2024-09-24T00:52:22.836656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Dentro do treinamento recebemos o input e o target, e uma visão interessante é que treinamos essa rede por teaching force, isso por que ao invés de propagar para a rede, no próximo espaço de tempo, o token previsto no espaço anterior, propagamos diretamente o token correto, isso por que, sem essa técnica, seria muito dificil o modelo começar a convergir, visto que iria gerar representações erradas duranto boa parte do treino. \n",
    "\n",
    "O treinamento funciona da seguinte forma:\n",
    "    Passasse uma sequência de max_sequence_len tokens:\n",
    "        A rede gera o primeiro hidden e o primeiro output usando o x0 e o h0\n",
    "        A rede usa o h1 (gerado pela rede avaliada no x0 + h0) e o x1 para gerar o h2 e o output 2.\n",
    "        O processo segue até que todas entradas tenham sido processadas.\n",
    "        Com isso somamos as losses e corrigimos o modelo.\"\"\"\n",
    "\n",
    "def train(input_tensor, target_tensor):\n",
    "\n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    loss = 0\n",
    "    for i in range(input_tensor.size(0)):\n",
    "        output, hidden = rnn(input_tensor[i], hidden)\n",
    "        loss += criterion(output, target_tensor[i].unsqueeze(0))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item() / input_tensor.size(0)"
   ],
   "id": "aa4c16a49b1c2494",
   "execution_count": 287,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:07:26.908804Z",
     "start_time": "2024-09-23T22:02:01.327592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fator_desconto = 10\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    total_loss = 0\n",
    "    for start_idx in tqdm(range(0, len(test_sentence) - max_sequence_len, fator_desconto*max_sequence_len)):\n",
    "        input_tensor, target_tensor = create_input(test_sentence, start_idx, max_sequence_len)\n",
    "        loss = train(input_tensor, target_tensor)\n",
    "        total_loss += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {total_loss}')"
   ],
   "id": "95d6fe2d75d7c99a",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  Atividades\n",
    "\n",
    "__1 - Escreva a função generate da classe RNN.__\n",
    "\n",
    "__2 - Escreva as funções de load_dataset, preprocess_data, tokenize_data e create_input.__\n",
    "\n",
    "__3 - Realize otimização de hiperparâmetros. Justifique a escolha dos hiperparâmetros otimizados e o espaço de busca definido.__\n",
    "\n",
    "__4 - Adicione uma Layer de Dropout na classe RNN. Treine o novo modelo e argumente sobre o impacto dessa alteração no modelo.__\n",
    "\n",
    "__5 - Adicione uma nova nn.Layer que recebe como input os vetores hidden e output combinados. Treine o novo modelo e argumente sobre o impacto dessa alteração no modelo.__\n",
    "\n",
    "__6 - Adicione uma função para printar uma geração de texto de no máximo 100 caracteres sempre que printar a loss do modelo.__\n",
    "\n",
    "__7 - Adicione uma função que calcula a perplexidade e printe com a loss.__\n",
    "\n",
    "__8 - Proponha três alterações no código e demonstre que melhorou o desempenho do modelo.__\n",
    "\n",
    "**Desafio**\n",
    "\n",
    "__1 - Desenvolva um modelo Word-Level utilizando LSTM.__\n",
    "- __Escrever a classe LSTM.__\n",
    "- __Escrever a função generate.__\n",
    "- __Otimizar o modelo.__\n",
    "- __Comparar com as outras abordagens acima__"
   ],
   "id": "ace91b341844dec2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Atividade 1",
   "id": "84fa99fe566aabef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:08:35.376769Z",
     "start_time": "2024-09-23T22:08:35.360621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Ideia da função generate:\n",
    "    \n",
    "    Temos a entrada como uma texto em formato string, podendo conter qualquer tamanho.\n",
    "    \n",
    "    Préprocessamos o texto com a mesma função usada no treino e tokenizamos o texto, onde cada carácter é um token e receberá sua representação númerica.\n",
    "    \n",
    "    Após isso, iteramos em todos caracteres do prompt para gerar o hidden-state inicial, não faz sentido olharmos para as previsões do modelo nessa etapa, justamente por que,\n",
    "    já possuimos o label, que seria o próximo token do prompt de entrada.\n",
    "    \n",
    "    Após pre-processar toda sequencia e obter a sua representação no vetor hidden, passamos a olhar para o output da rede. Esse output vem no formato de uma distribuição de probabilidade dado pela softmax. A softmax gera uma função densidade de probabilidade onde a soma dos valores é igual a 1 e não existe nenhum valor menor que 0, se integrarmos a softmax de -inf a +inf obteremos 1 como resposta. Para nosso modelo, estamos usando a LogSoftmax, onde retiramos o log do valor gerado pela softmax, como a softmax só produz valores entre 0 e 1, todos nossos tokens terão um valor negativo associado, para facilitar a explicação, vamos assumir que estamos usando puramente a softmax, visto que o efeito é semelhante.\n",
    "    \n",
    "    Com um valor referente a uma probabilidade/score de cada token, pegamos o token de maior probabilidade/score caso o aleatorio seja igual a False e o adicionamos ao prompt, repetindo esse processo, até gerar 100 tokens.\n",
    "   \"\"\"\n",
    "\n",
    "def generate_rnn(rnn, prompt=' ', tamanho_resposta=100, aleatorio = True ,top_k=3):\n",
    "    rnn.eval() # Modo de avaliação do modelo, desliga dropout e etc\n",
    "    oculto = rnn.initHidden() # Inicialização do hidden\n",
    "    prompt_processado = preprocess_data(prompt)\n",
    "    prompt_tensor = torch.tensor([vocab[i] for i in list(prompt_processado)], dtype=torch.int16) # Prompt pré processado e tokenizado\n",
    "\n",
    "    for token in prompt_tensor:\n",
    "        one_hot = torch.zeros(1, len(vocab))\n",
    "        one_hot[0][token] = 1 # Gerar a representação em one-hot-encoding para cada token\n",
    "\n",
    "    for _ in range(tamanho_resposta):\n",
    "        y_pred, oculto = rnn(one_hot, oculto) # Passar o token e o hidden para a rede, que devolve o novo hidden e uma distribuição de probabilidades\n",
    "        if not aleatorio: # Escolhe sempre o token de maior probabilidade\n",
    "            token_previsto = torch.argmax(y_pred, dim=1) # Escolhe o token previsto como o de maior probabilidade\n",
    "        else: # A es\n",
    "            y_pred = torch.softmax(y_pred, dim=1)\n",
    "            top_k_valores, top_k_indices = torch.topk(y_pred, top_k)\n",
    "            top_k_valores = top_k_valores / torch.sum(top_k_valores)\n",
    "            token_previsto = top_k_indices[0, torch.multinomial(top_k_valores, 1)]\n",
    "\n",
    "        print(vocab_ix_to_char[token_previsto.item()], end='')\n",
    "        one_hot = torch.zeros(1, len(vocab))\n",
    "        one_hot[0][token_previsto] = 1"
   ],
   "id": "71c8bb49c2c1d421",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:08:35.567636Z",
     "start_time": "2024-09-23T22:08:35.546665Z"
    }
   },
   "cell_type": "code",
   "source": "generate_rnn(rnn, prompt='when', tamanho_resposta=100, aleatorio=False)",
   "id": "e854aeff1a7835d5",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:08:36.303157Z",
     "start_time": "2024-09-23T22:08:36.293977Z"
    }
   },
   "cell_type": "code",
   "source": "generate_rnn(rnn, prompt='when', tamanho_resposta=100, aleatorio=True)",
   "id": "26a5038624cc940a",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### BALANCEANDO SEQUÊNCIAS",
   "id": "1ebfba70bfd78311"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:49:58.354719Z",
     "start_time": "2024-09-24T00:49:57.897474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"É possível perceber que mesmo conseguindo gerar palavras através de caracteres temos uma repetição alta. Em outros testes, foi percebido que o modelo tende a repetir bastante palavras como the, to e etc, que são de alta frequência no texto, vamos verificar essa hipótese:\"\"\"\n",
    "\n",
    "texto = load_dataset('obama-data.txt')\n",
    "texto = preprocess_data(texto)\n",
    "palavras = texto.split()\n",
    "contagem_palavras = Counter(palavras)\n",
    "\n",
    "\n",
    "df_contagem = pd.DataFrame(contagem_palavras.items(), columns=['Palavra', 'Frequencia'])\n",
    "df_contagem = df_contagem.sort_values(by='Frequencia', ascending=False)\n",
    "media_frequencia = df_contagem['Frequencia'].mean()\n",
    "df_contagem_limited = df_contagem.head(20)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(df_contagem_limited['Palavra'], df_contagem_limited['Frequencia'], color='skyblue')\n",
    "plt.axhline(y=media_frequencia, color='red', linestyle='--', label=f'Média: {media_frequencia:.2f}')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Palavras')\n",
    "plt.ylabel('Frequência')\n",
    "plt.title('Top 20 Palavras mais Frequentes no Texto')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "id": "38c7a6be5a5892dd",
   "execution_count": 272,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:49:58.361591Z",
     "start_time": "2024-09-24T00:49:58.356516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_20 = sum(list(df_contagem_limited['Frequencia']))\n",
    "todos = sum(list(df_contagem['Frequencia'])) - top_20\n",
    "\n",
    "print('Palavras no top 20:', top_20)\n",
    "print('Restante das palavras:', todos)"
   ],
   "id": "c4404d3826a64236",
   "execution_count": 273,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A linha em vermelho é a média de frequência das palavras, com isso, podemos notar que o modelo corrige muitoo mais certas palavras do que outras, o que pode prejudicar nosso treinamento. Ainda podemos ver que praticamente 1/3 das palavras estão no top 20, o que pode ser um problema para o modelo, visto que ele tende a repetir palavras de alta frequência.",
   "id": "3b836f544f2b510f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:50:00.934277Z",
     "start_time": "2024-09-24T00:50:00.930026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''A ideia é que caso uma palavra seja stopword, adicionamos 50% de chance de removê-lá do texto, caso ela permaneça nas mais frequentes'''\n",
    "def augment_text_by_word_freq(text, word_counts, prob_map, stopword_prob=0.98):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    palavras_prob = {}\n",
    "    start = 0\n",
    "    for top_n, prob in prob_map.items():\n",
    "        palavras_frequentes = word_counts.most_common(top_n)[start:top_n]\n",
    "        palavras_prob.update({palavra: prob for palavra, _ in palavras_frequentes})\n",
    "        start = top_n\n",
    "\n",
    "    palavras_texto = text.split()\n",
    "    augmented_text = []\n",
    "\n",
    "    for palavra in palavras_texto:\n",
    "        if palavra in stop_words:\n",
    "            if random.uniform(0, 1) < stopword_prob:\n",
    "                continue\n",
    "                \n",
    "        elif palavra in palavras_prob:\n",
    "            if random.uniform(0, 1) < palavras_prob[palavra]:\n",
    "                continue\n",
    "\n",
    "        augmented_text.append(palavra)\n",
    "\n",
    "    return ' '.join(augmented_text)"
   ],
   "id": "bd2923acb6a3825e",
   "execution_count": 274,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:50:04.767201Z",
     "start_time": "2024-09-24T00:50:04.372617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_sentence = load_dataset('obama-data.txt')\n",
    "\n",
    "test_sentence = preprocess_data(test_sentence)\n",
    "\n",
    "prob_map = {\n",
    "    14: 0.5,  # Para as 100 palavras mais frequentes, 90% de chance de remoção\n",
    "    20: 0.2,  # Para as próximas 100 palavras, 50% de chance de remoção\n",
    "    50: 0.1   # Para as próximas 100 palavras, 20% de chance de remoção\n",
    "}\n",
    "test_sentence = augment_text_by_word_freq(test_sentence, contagem_palavras, prob_map, stopword_prob=0.95)"
   ],
   "id": "377c7fe6bf7f2001",
   "execution_count": 275,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:50:05.013989Z",
     "start_time": "2024-09-24T00:50:04.783734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"É possível perceber que mesmo conseguindo gerar palavras através de caracteres temos uma repetição alta. Em outros testes, foi percebido que o modelo tende a repetir bastante palavras como the, to e etc, que são de alta frequência no texto, vamos verificar essa hipótese:\"\"\"\n",
    "\n",
    "palavras = test_sentence.split()\n",
    "\n",
    "contagem_palavras = Counter(palavras)\n",
    "\n",
    "df_contagem = pd.DataFrame(contagem_palavras.items(), columns=['Palavra', 'Frequencia'])\n",
    "df_contagem = df_contagem.sort_values(by='Frequencia', ascending=False)\n",
    "media_frequencia = df_contagem['Frequencia'].mean()\n",
    "\n",
    "df_contagem_limited = df_contagem.head(20)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(df_contagem_limited['Palavra'], df_contagem_limited['Frequencia'], color='skyblue')\n",
    "plt.axhline(y=media_frequencia, color='red', linestyle='--', label=f'Média: {media_frequencia:.2f}')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Palavras')\n",
    "plt.ylabel('Frequência')\n",
    "plt.title('Top 20 Palavras mais Frequentes no Texto')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "id": "4052184fe3b4e35f",
   "execution_count": 276,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:50:06.108781Z",
     "start_time": "2024-09-24T00:50:06.020314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_sentence, vocab = tokenize_data(test_sentence)\n",
    "\n",
    "vocab_len = len(vocab)\n",
    "vocab_ix_to_char = {v: k for k, v in vocab.items()}"
   ],
   "id": "52c3960806dea70b",
   "execution_count": 277,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Retreinando a rnn",
   "id": "26f76a90b3d3f583"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:32:31.014031Z",
     "start_time": "2024-09-23T22:30:08.332390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fator_desconto = 10\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    total_loss = 0\n",
    "    for start_idx in tqdm(range(0, len(test_sentence) - max_sequence_len, fator_desconto*max_sequence_len)):\n",
    "        input_tensor, target_tensor = create_input(test_sentence, start_idx, max_sequence_len)\n",
    "        loss = train(input_tensor, target_tensor)\n",
    "        total_loss += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {total_loss}')\n",
    "\n",
    "generate_rnn(rnn, prompt='when', tamanho_resposta=100)"
   ],
   "id": "b8d4fdbcfcf1de0e",
   "execution_count": 60,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:32:42.380157Z",
     "start_time": "2024-09-23T22:32:42.369098Z"
    }
   },
   "cell_type": "code",
   "source": "generate_rnn(rnn, prompt='when', tamanho_resposta=100, aleatorio=False)",
   "id": "9b45859e6465dd50",
   "execution_count": 61,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:32:47.270288Z",
     "start_time": "2024-09-23T22:32:47.254582Z"
    }
   },
   "cell_type": "code",
   "source": "generate_rnn(rnn, prompt='when', tamanho_resposta=100, aleatorio=True)",
   "id": "eada4a4c266de063",
   "execution_count": 62,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Atividade 3 (otimização de hiperparâmetros)",
   "id": "29233fe57e1b4a30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A rnn fornecida possui dois principais hiperparêmetros para otimização que são o hidden_size e o learning_rate. \n",
    "\n",
    "  -Hidden_size: o hidden_size é o tamanho do vetor que representa a sequência, além disso, temos duas lineares dentro da rnn, a i2h que é responsável por gerar o próximo hidden_state, ou seja, sua saída possui hidden_size valores, e portanto, são hidden_size neurônios presentes nessa camada. Com isso, quanto maior o hidden_size, mais neurônios teremos na rede e consequentemente um maior grau de liberdade do modelo, além de um maior vetor para representação do hidden_state\n",
    "  \n",
    "  -Já a learning_rate é o tamanho do passo que o otimizador dá para corrigir os pesos da rede, visto que na atualização de pesos do modelo cada peso w passará pela seguinte equação w = w - grad*learning_rate, assim ,um learning_rate muito pequeno pode fazer com que o modelo demore muito para convergir, enquanto um muito grande pode fazer com que o modelo não consiga convergir, ou até mesmo divergir, por sair \"pulando\" no espaço da função de perca de modo que nunca vá em direção a um mínimo, seja ele local ou global.\n",
    "  \n",
    "Para exploração dessas váriaveis temos a seguinte intuição:\n",
    "  -Hidden_size: Quanto maior o hidden_size, mais informação a rede pode armazenar, porém, também mais difícil de treinar, visto que mais neurônios significa mais parâmetros a serem ajustados, além disso, um hidden_size muito grande pode fazer com que o modelo se sobreajuste aos dados, visto que terá muita capacidade de representação. Por outro lado, um hidden_size muito pequeno pode fazer com que o modelo não consiga aprender a representação da sequência, visto que não terá capacidade suficiente para armazenar informação.\n",
    "    \n",
    "  -Learning_rate: Quanto maior o learning_rate, mais rápido o modelo converge, porém, também mais fácil de divergir, visto que o otimizador pode \"pular\" o mínimo local ou global. Um learning_rate muito pequeno pode fazer com que o modelo demore muito para convergir, ou até mesmo não convergir, visto que o passo é muito pequeno, além disso, pode fazer com que o modelo fique preso em mínimos locais."
   ],
   "id": "a589a993ee2d4a89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-20T01:10:08.922972Z",
     "start_time": "2024-09-20T01:10:08.920840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''Com isso, vamos otimizar esses hiperparâmetros, para isso, vamos definir um espaço de busca para cada hiperparâmetro e realizar uma busca em grade (grid_search), onde testaremos todas combinações possíveis de hiperparâmetros e escolheremos a melhor combinação. Os valores foram escolhidos de maneira árbritária, com base em conhecimento prévio e intuição, além disso, testaremos os hiperparâmetros em uma parcela reduzida dos dados fornecidos, devido ao alto custo computacional.'''\n",
    "\n",
    "n_hidden_options = [32, 64, 128, 256]\n",
    "learning_rate_options = [0.1, 0.01, 0.001, 0.0001]"
   ],
   "id": "d94fcf0da642baaf",
   "execution_count": 1213,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T23:15:39.585568Z",
     "start_time": "2024-09-19T23:15:39.583283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_epochs = 5\n",
    "print_every = 1\n",
    "max_sequence_len = 25\n",
    "fator_desconto = 100"
   ],
   "id": "7ca515804d79374",
   "execution_count": 805,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T23:15:39.787667Z",
     "start_time": "2024-09-19T23:15:39.785535Z"
    }
   },
   "cell_type": "code",
   "source": "results = []",
   "id": "1a29a42a1df2ea2a",
   "execution_count": 806,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T23:22:08.876910Z",
     "start_time": "2024-09-19T23:16:53.204385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for n_hidden in n_hidden_options:\n",
    "    for learning_rate in learning_rate_options:\n",
    "        print(f'\\nTreinando com n_hidden={n_hidden} e learning_rate={learning_rate}')\n",
    "\n",
    "        rnn = RNN(vocab_len, n_hidden, vocab_len)\n",
    "        criterion = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            epoch_loss = 0\n",
    "            for start_idx in range(0, len(test_sentence) - max_sequence_len, fator_desconto * max_sequence_len):\n",
    "                input_tensor, target_tensor = create_input(test_sentence, start_idx, max_sequence_len)\n",
    "                loss = train(input_tensor, target_tensor)\n",
    "                epoch_loss += loss\n",
    "\n",
    "            if epoch % print_every == 0:\n",
    "                print(f'Epoch: {epoch}, Loss: {epoch_loss}')\n",
    "            total_loss += epoch_loss\n",
    "\n",
    "        results.append({\n",
    "            'n_hidden': n_hidden,\n",
    "            'learning_rate': learning_rate,\n",
    "            'total_loss': total_loss\n",
    "        })\n",
    "        print()"
   ],
   "id": "463e7d17f959bf22",
   "execution_count": 808,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T23:31:28.970349Z",
     "start_time": "2024-09-19T23:31:28.729916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "unique_n_hidden = df_results['n_hidden'].unique()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for n_hidden in unique_n_hidden:\n",
    "    subset = df_results[df_results['n_hidden'] == n_hidden]\n",
    "    plt.plot(subset['learning_rate'], subset['total_loss'], label=f'n_hidden={n_hidden}', marker='o')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Total Loss (Log Scale)')\n",
    "plt.title('Total Loss vs Learning Rate for Different n_hidden Values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ],
   "id": "dd32b63739ec9d4f",
   "execution_count": 817,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T23:35:50.533851Z",
     "start_time": "2024-09-19T23:35:50.188577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for n_hidden in unique_n_hidden:\n",
    "    subset = df_results[df_results['n_hidden'] == n_hidden]\n",
    "    plt.plot(subset['learning_rate'], subset['total_loss'], label=f'n_hidden={n_hidden}', marker='o')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylim(1.5*10**4, 2.5*10**4)  # Ajuste para melhorar a visualização no início do gráfico\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Total Loss (Log Scale)')\n",
    "plt.title('Total Loss vs Learning Rate for Different n_hidden Values (Adjusted Y Scale)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ],
   "id": "ff6d055edd8f1be6",
   "execution_count": 826,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Assim a combinação de valores queresultou na menor perca foi n_hidden = 128 e learning_rate = 10**(-3)",
   "id": "d09946980e7fcb70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Atividade 4",
   "id": "91b4cc67d0998bf5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:19:43.083785Z",
     "start_time": "2024-09-23T23:19:43.078928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Adicionando dropout:\n",
    "    \n",
    "    Em redes neurais, existe um teorema que uma mlp, se suficientemente grande, é um aproximador universal de funções, contudo, na prática, não sabemos exatamente a função que descreve o fenômeno modelado, estimamos ela através dos dados, onde é possível ter um modelo com alta variância. Isso pode ser problemático pois indica que o modelo se sobreajustou aqueles dados,que não necessariamente descrevem a função alvo ou apresentam falta de informação, gerando um modelo muito bom, porém apenas para aquele conjunto específico que tivemos acesso.\n",
    "    \n",
    "    Com isso, há a utilização do dropout a fim de trazer uma regularização a rede, onde, de maneira aleatória, alguns neurônios são desligados durante o treinamento, fazendo com que a rede possua um universo de funções alcançaveis reduzido, e consequentemente 'aprenda' a não depender de um único neurônio, ou um conjunto específico deles, para fazer uma predição. Essa dificultação diminui a chance de um overfitting, e possui uma visão interessante onde podemos visualizar a técnica de dropout como um \"ensemble\" de redes neurais, visto que a cada foward pass, provavelmente estamos utilizando uma configuração de rede diferente, com neurônios diferentes desligados.\n",
    "    \n",
    "    Assim, espera-se um modelo com melhor capacidade de generalização.\"\"\"\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob=0.5):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob) # Adição da camada de dropout\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input_combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        \n",
    "        hidden = F.tanh(hidden)\n",
    "\n",
    "        hidden = self.dropout(hidden) # Adição do dropout\n",
    "        \n",
    "        output = self.i2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ],
   "id": "239d79bbb748ad59",
   "execution_count": 180,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:19:43.239211Z",
     "start_time": "2024-09-23T23:19:43.236721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_hidden = 128\n",
    "dropout_prob = 0.3\n",
    "rnn = RNN(vocab_len, n_hidden, vocab_len, dropout_prob)"
   ],
   "id": "8e1f288d1cfe4276",
   "execution_count": 181,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:19:43.435900Z",
     "start_time": "2024-09-23T23:19:43.432307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 0.001\n",
    "n_epochs = 5\n",
    "print_every = 1\n",
    "max_sequence_len = 25\n",
    "fator_desconto = 10"
   ],
   "id": "1f18f63919ea5d1a",
   "execution_count": 182,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:19:43.704923Z",
     "start_time": "2024-09-23T23:19:43.702405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)"
   ],
   "id": "895e7a46eb4aef68",
   "execution_count": 183,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:22:16.366228Z",
     "start_time": "2024-09-23T23:19:44.024761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    total_loss = 0\n",
    "    for start_idx in tqdm(range(0, len(test_sentence) - max_sequence_len, fator_desconto*max_sequence_len)):\n",
    "        input_tensor, target_tensor = create_input(test_sentence, start_idx, max_sequence_len)\n",
    "        loss = train(input_tensor, target_tensor)\n",
    "        total_loss += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {total_loss}')"
   ],
   "id": "d54a5342868695a",
   "execution_count": 184,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:22:17.410453Z",
     "start_time": "2024-09-23T23:22:17.394388Z"
    }
   },
   "cell_type": "code",
   "source": "generate_rnn(rnn, prompt='when', tamanho_resposta=100, aleatorio=False)",
   "id": "1d8f2761313ccbcf",
   "execution_count": 185,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:22:17.800189Z",
     "start_time": "2024-09-23T23:22:17.787168Z"
    }
   },
   "cell_type": "code",
   "source": "generate_rnn(rnn, prompt='when', tamanho_resposta=100)",
   "id": "9d3d7334e8d32099",
   "execution_count": 186,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Atividade 5",
   "id": "fe2756295595b14b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:36:44.845140Z",
     "start_time": "2024-09-23T23:36:44.838824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"A mudança cria um loop de feedback onde a saída em cada passo de tempo influencia o estado oculto para o próximo passo. Isso pode ser benéfico em alguns aspectos, por exemplo, corrigimos diretamente o erro que a camada anterior causa ao propagar um token errado para a rede no próximo instante de tempo, além disso, estamos dando um maior grau de liberdade a rede, que agora possui mais parâmetros, o que também pode ser benéfico, mas pode também causar um overfitting. Além disso, estamos adicionando um caminho no gradiente, o que pode ser problemático, visto que o gradiente pode explodir ou sumir, e com um caminho a mais, a chance disso acontecer é ainda maior.\"\"\"\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "        self.ho2h = nn.Linear(hidden_size + output_size, hidden_size) # Nova camada que recebe hidden e output concatenados\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input_combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        hidden = torch.tanh(hidden)\n",
    "\n",
    "        output = self.i2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        ho_combined = torch.cat((hidden, output), 1)\n",
    "        hidden = self.ho2h(ho_combined)\n",
    "        hidden = torch.tanh(hidden)\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n"
   ],
   "id": "884c86935bb2ac45",
   "execution_count": 212,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:36:45.086589Z",
     "start_time": "2024-09-23T23:36:45.083559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_hidden = 128\n",
    "rnn = RNN(vocab_len, n_hidden, vocab_len)"
   ],
   "id": "16e04b696ffa7c67",
   "execution_count": 213,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:36:45.400788Z",
     "start_time": "2024-09-23T23:36:45.396347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rate = 0.0005\n",
    "n_epochs = 5\n",
    "print_every = 1"
   ],
   "id": "c9c2769a8ec0e13",
   "execution_count": 214,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:36:45.723643Z",
     "start_time": "2024-09-23T23:36:45.720807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)"
   ],
   "id": "b985fffb610e1ae2",
   "execution_count": 215,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:39:55.346845Z",
     "start_time": "2024-09-23T23:36:46.115887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    total_loss = 0\n",
    "    for start_idx in tqdm(range(0, len(test_sentence) - max_sequence_len, fator_desconto*max_sequence_len)):\n",
    "        input_tensor, target_tensor = create_input(test_sentence, start_idx, max_sequence_len)\n",
    "        loss = train(input_tensor, target_tensor)\n",
    "        total_loss += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {total_loss}')"
   ],
   "id": "a7daea01e13194b2",
   "execution_count": 216,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:40:00.772116Z",
     "start_time": "2024-09-23T23:40:00.759964Z"
    }
   },
   "cell_type": "code",
   "source": "generate_rnn(rnn, prompt='when', tamanho_resposta=100, aleatorio = False)",
   "id": "4a236029fe22f243",
   "execution_count": 217,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:40:03.945639Z",
     "start_time": "2024-09-23T23:40:03.932529Z"
    }
   },
   "cell_type": "code",
   "source": "generate_rnn(rnn, prompt='when', tamanho_resposta=100, aleatorio = True)",
   "id": "b7e4b4d6c1fa996c",
   "execution_count": 218,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Mesmo gerando palavras, não tivemos um bom resultado na geração de texto, estamos maximizando as probabilidades, mas não de maneira suficiente para que o token mais provavel seja o correto",
   "id": "77f1bb6ed7a75bec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Atividade 6",
   "id": "1b22ae1f7c0728a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:43:43.523765Z",
     "start_time": "2024-09-23T23:40:35.789218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    total_loss = 0\n",
    "    for start_idx in tqdm(range(0, len(test_sentence) - max_sequence_len, fator_desconto*max_sequence_len)):\n",
    "        input_tensor, target_tensor = create_input(test_sentence, start_idx, max_sequence_len)\n",
    "        loss = train(input_tensor, target_tensor)\n",
    "        total_loss += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {total_loss}')\n",
    "        generate_rnn(rnn, prompt='when', tamanho_resposta=100)"
   ],
   "id": "cd52f2506c7b66cf",
   "execution_count": 219,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Atividade 7",
   "id": "48a536caffc3d19a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "$$ \\text{perplexity} = \\left( \\prod_{i=1}^{n} \\frac{1}{P(w_i | w_1, w_2, \\ldots, w_{i-1})} \\right)^{\\frac{1}{n}} $$",
   "id": "c6fa2ee776b3f09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Explicação da perplexidade",
   "id": "a1b41c42cac462e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:45:57.502877Z",
     "start_time": "2024-09-23T23:45:57.494094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Conforme os slides passados em aula, a perplexidade serve para avaliação de um language model, a alto nível, a perplexidade indica o quão 'surpreso' o modelo está em relação a uma sequência de tokens. A perplexidade é uma medida de incerteza, onde quanto menor a perplexidade, melhor o modelo, visto que o modelo está mais confiante em suas predições, e consequentemente, a sequência de tokens é mais provável de acordo com o modelo. \n",
    "\n",
    "    Matematicamente a intuição acima faz sentido, se enxergarmos a saída da softmax como probabilidades, e o calculo da perplexidade como sendo a raiz n-ésima do produtorio de 1 dividido pela probabilidade do token dado os tokens anteriores para cada token na sequência, temos que se a probabilidade daquele token for baixa, 1 / por um número bem pequeno resulta em um número muito grande, por outro lado, 1 dividido por um número próximo de 1 resulta em um número próximo de 1, assim, para cada token, quanto maior a probabilidade que o modelo dá para aquele token de fato menor a perplexidade.\n",
    "    \n",
    "    Na implementação abaixo, seria interessante obter probabilidades, como o modelo retorna o Log da softmax, iremos realizar a operação inversa (exponenciação) para voltar para o resultado da softmax, que irá nos gerar uma distribuição de probabilidade, assim, fazendo mais sentido para o cálculo da perplexidade.\n",
    "    \n",
    "    Outro ponti é que adicionamos um pequeno valor a probabilidade de cada token, isso por que é possivel (embora não seja muito provável) que um token tenha probabilidade 0 ou algo muito próximo disso, podendo causar uma divisão por 0.\"\"\"\n",
    "\n",
    "def perplexidade(modelo, texto):\n",
    "\n",
    "    modelo.eval()\n",
    "\n",
    "    hidden = modelo.initHidden()\n",
    "    texto = preprocess_data(texto)\n",
    "    texto = torch.tensor([vocab[i] for i in list(texto)], dtype=torch.long)\n",
    "    \n",
    "    perplexidade = 1\n",
    "    produtorio_probs = 1\n",
    "    for i in range(len(texto) - 1): #Sempre usamos os n ultimos tokens para predizer o atual\n",
    "        token_atual = texto[i]\n",
    "        proximo_token = texto[i+1]\n",
    "\n",
    "        one_hot = torch.zeros(1, vocab_len)\n",
    "        one_hot[0][token_atual] = 1\n",
    "\n",
    "        y_pred, hidden = modelo.forward(one_hot, hidden)\n",
    "        y_pred = torch.exp(y_pred) # Transformar log_softmax em softmax\n",
    "\n",
    "        p_token = (y_pred[0][proximo_token]).item() #pegar probabilidade do token correto (proximo token)\n",
    "        produtorio_probs *= (1 / (p_token + + 1e-12)) # Adicionando um valor pequeno para evitar divisão por 0 (mesmo que altamente improvável)\n",
    "\n",
    "    n = len(texto)\n",
    "    if n > 0:\n",
    "        perplexidade = produtorio_probs ** (1 / float(n)) # Tirando a raiz enesima\n",
    "\n",
    "    return perplexidade"
   ],
   "id": "e85ab3568d115f00",
   "execution_count": 220,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:45:58.087215Z",
     "start_time": "2024-09-23T23:45:58.081089Z"
    }
   },
   "cell_type": "code",
   "source": "perplexidade(rnn, 'when')",
   "id": "7924f3c20914ce65",
   "execution_count": 221,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:46:00.170195Z",
     "start_time": "2024-09-23T23:46:00.121388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Como vamos plotar a perplexidade junto com a loss, não faz sentido avaliar ela no conjunto de treino, pois a mesma irá se comportar de maneira muito similar a loss, visto que a perplexidade é uma medida de incerteza, e a loss é uma medida de erro, váriaveis altamente correlacionadas, assim, vamos avaliar a perplexidade no conjunto de teste, para medir a qualidade do modelo.\"\"\"\n",
    "\n",
    "shakespeare_data = load_dataset('shakespeare-data.txt')\n",
    "shakespeare_data = preprocess_data(shakespeare_data)"
   ],
   "id": "d3f34528c43d3ed2",
   "execution_count": 222,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:46:00.371167Z",
     "start_time": "2024-09-23T23:46:00.358142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_hidden = 64\n",
    "rnn = RNN(vocab_len, n_hidden, vocab_len)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)"
   ],
   "id": "b3444d0613970c9a",
   "execution_count": 223,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:46:00.932991Z",
     "start_time": "2024-09-23T23:46:00.929748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''Vamos usar a perplexidade como comparativo para os modelos'''\n",
    "\n",
    "models_perplexidade = {}"
   ],
   "id": "c3e420373c70d919",
   "execution_count": 224,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:48:52.605224Z",
     "start_time": "2024-09-23T23:46:01.532511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"Para o cáclulo da perplexidade, vamos adotar sequencias com o mesmo tamanho do max_sequence_len, visto que o modelo foi treinado com sequencias de tamanho fixo, e assim, a perplexidade será calculada para cada sequencia de tamanho max_sequence_len. Para exibição, faremos a média da perplexidade ao longo da época.\"\"\"\n",
    "\n",
    "perplex = []\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    total_loss = 0\n",
    "    perplexidade_total = 0\n",
    "    count = 0\n",
    "    for start_idx in tqdm(range(0, len(test_sentence) - max_sequence_len, fator_desconto*max_sequence_len)):\n",
    "        input_tensor, target_tensor = create_input(test_sentence, start_idx, max_sequence_len)\n",
    "        loss = train(input_tensor, target_tensor)\n",
    "        total_loss += loss\n",
    "        \n",
    "        perplexidade_total += perplexidade(rnn, shakespeare_data[start_idx: start_idx + max_sequence_len])\n",
    "        count +=1\n",
    "    if epoch % print_every == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {total_loss}')\n",
    "        print(f'Perplexidade nos dados de teste: {(perplexidade_total / count):.4f}')\n",
    "        perplex.append(perplexidade_total / count)\n",
    "        generate_rnn(rnn, prompt='when', tamanho_resposta=100)\n",
    "        \n",
    "models_perplexidade['RNN sem alteração'] = perplex"
   ],
   "id": "dd51fca127cb9c02",
   "execution_count": 225,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Atividade 8",
   "id": "7bbec7a78a4ff3a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Alteração 1** - Adição de embeddings para representação de um caracter",
   "id": "3bc457315f06d51d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:24:19.405923Z",
     "start_time": "2024-09-24T00:24:19.396717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''Adicionando embeddings'''\n",
    "\n",
    "embedding_dim = 128\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, embedding_dim, dropout_prob=0.5):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(input_size, embedding_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        index = torch.nonzero(input).squeeze()\n",
    "        index = index[-1]\n",
    "        index = torch.tensor(index.item())\n",
    "\n",
    "        embs = self.embeddings(index)\n",
    "        embs = embs.unsqueeze(0)\n",
    "\n",
    "        input_combined = torch.cat((embs, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        hidden = F.tanh(hidden)\n",
    "\n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        output = self.i2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ],
   "id": "4b306d2bf24dd6a1",
   "execution_count": 226,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:24:19.976138Z",
     "start_time": "2024-09-24T00:24:19.947430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rnn = RNN(vocab_len, n_hidden, vocab_len, embedding_dim)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "n_hidden = 128\n",
    "learning_rate = 0.0005\n",
    "n_epochs = 5\n",
    "print_every = 1\n",
    "max_sequence_len = 25\n",
    "rnn"
   ],
   "id": "e4d85171c2f81967",
   "execution_count": 227,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:27:49.331108Z",
     "start_time": "2024-09-24T00:24:21.077103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "perplex = []\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    total_loss = 0\n",
    "    perplexidade_total = 0\n",
    "    count = 0\n",
    "    for start_idx in tqdm(range(0, len(test_sentence) - max_sequence_len, fator_desconto*max_sequence_len)):\n",
    "        input_tensor, target_tensor = create_input(test_sentence, start_idx, max_sequence_len)\n",
    "        loss = train(input_tensor, target_tensor)\n",
    "        total_loss += loss\n",
    "\n",
    "        perplexidade_total += perplexidade(rnn, shakespeare_data[start_idx: start_idx + max_sequence_len])\n",
    "        count +=1\n",
    "    if epoch % print_every == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {total_loss}')\n",
    "        print(f'Perplexidade nos dados de teste: {(perplexidade_total / count):.4f}')\n",
    "        perplex.append(perplexidade_total / count)\n",
    "        generate_rnn(rnn, prompt='when', tamanho_resposta=100)\n",
    "\n",
    "models_perplexidade['RNN alteração 1'] = perplex"
   ],
   "id": "24432aabf92769d0",
   "execution_count": 228,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:28:15.682346Z",
     "start_time": "2024-09-24T00:28:15.651054Z"
    }
   },
   "cell_type": "code",
   "source": "generate_rnn(rnn, prompt='when', tamanho_resposta=100)",
   "id": "cf336e94e77ea94a",
   "execution_count": 229,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:28:18.651512Z",
     "start_time": "2024-09-24T00:28:18.639844Z"
    }
   },
   "cell_type": "code",
   "source": "generate_rnn(rnn, prompt='when', tamanho_resposta=100, aleatorio=True)",
   "id": "dd55142b8a62aec7",
   "execution_count": 230,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**ALteração 2**: Inicialização de pesos:\n",
    "    Em sequencias muito longas, o gradiente pode explodir ou desaparecer, isso agravado em uma rede recorrente, justamente por que em cada step de tempo temos a mesma rede compartilhando os mesmos pesos, e assim diminuindo o grau de liberdade do modelo em comparação a uma mlp de multiplas camadas, assim, é mais \"fácil\" corrigir no inicio da sequencia, causando o problema de dependencia de longo prazo das redes neurais recorrentes vanilla. Uma das formas de tentar mitigar a explosão/ desaparecimento de gradiente nessas redes é com uma inicialização adequada dos pesos. A intuição é que a inicialização adequada normaliza a variância dos pesos, e assim, evitando uma alta amplitude."
   ],
   "id": "fae32747253f140c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T01:25:49.864570Z",
     "start_time": "2024-09-24T01:25:49.852915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_dim = 128\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, embedding_dim, dropout_prob=0.5):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(input_size, embedding_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        index = torch.nonzero(input).squeeze()\n",
    "        index = index[-1]\n",
    "        index = torch.tensor(index.item())\n",
    "\n",
    "        embs = self.embeddings(index)\n",
    "        embs = embs.unsqueeze(0)\n",
    "\n",
    "        input_combined = torch.cat((embs, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        hidden = F.tanh(hidden)\n",
    "\n",
    "        hidden = self.dropout(hidden)\n",
    "\n",
    "        output = self.i2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    \n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.i2h.weight) # Inicialização Xavier para as camadas lineares\n",
    "        nn.init.xavier_uniform_(self.i2o.weight)\n",
    "        nn.init.zeros_(self.i2h.bias)\n",
    "        nn.init.zeros_(self.i2o.bias)\n",
    "\n",
    "        nn.init.uniform_(self.embeddings.weight, -1.0, 1.0)"
   ],
   "id": "cb657e178f7e5a89",
   "execution_count": 307,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:50:51.791804Z",
     "start_time": "2024-09-24T00:50:51.785761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rnn = RNN(vocab_len, n_hidden, vocab_len, embedding_dim)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "n_hidden = 128\n",
    "learning_rate = 0.0005\n",
    "n_epochs = 5\n",
    "print_every = 1\n",
    "max_sequence_len = 20\n",
    "rnn"
   ],
   "id": "815440c9f50ced4f",
   "execution_count": 283,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:34:26.309648Z",
     "start_time": "2024-09-24T00:30:37.169532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "perplex = []\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    total_loss = 0\n",
    "    perplexidade_total = 0\n",
    "    count = 0\n",
    "    for start_idx in tqdm(range(0, len(test_sentence) - max_sequence_len, fator_desconto*max_sequence_len)):\n",
    "        input_tensor, target_tensor = create_input(test_sentence, start_idx, max_sequence_len)\n",
    "        loss = train(input_tensor, target_tensor)\n",
    "        total_loss += loss\n",
    "\n",
    "        perplexidade_total += perplexidade(rnn, shakespeare_data[start_idx: start_idx + max_sequence_len])\n",
    "        count +=1\n",
    "    if epoch % print_every == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {total_loss}')\n",
    "        print(f'Perplexidade nos dados de teste: {(perplexidade_total / count):.4f}')\n",
    "        perplex.append(perplexidade_total / count)\n",
    "        generate_rnn(rnn, prompt='when', tamanho_resposta=100)\n",
    "\n",
    "models_perplexidade['RNN alteração 2'] = perplex"
   ],
   "id": "3e8b0def4171443e",
   "execution_count": 233,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Alteração 3** Além disso, um grande limitante do modelo é a maneira a qual dividimos os dados, visto que definimos um tamanho fixo de sequência e amostramos ao longo do texto, sem que nos importemos com a qualidade dessas amostras. Assim, é possível que o modelo esteja sendo treinado em cima de uma sequencia que não faz sentido (uma palavra cortada por exemplo). Por isso, implementaremos um método de amostragem mais inteligente e que façam sentido, ou seja, que não cortem palavras.",
   "id": "5d1cfe7598b0f28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T00:56:39.227136Z",
     "start_time": "2024-09-24T00:56:39.211707Z"
    }
   },
   "cell_type": "code",
   "source": "vocab[' ']",
   "id": "553543ff0e717e7e",
   "execution_count": 289,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T01:08:47.661154Z",
     "start_time": "2024-09-24T01:08:47.648573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_hidden = 128\n",
    "embedding_dim = 128\n",
    "rnn = RNN(vocab_len, n_hidden, vocab_len, embedding_dim)\n",
    "\n",
    "learning_rate = 0.0005\n",
    "n_epochs = 5\n",
    "print_every = 1\n",
    "max_sequence_len = 25\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "rnn"
   ],
   "id": "1b1edf4c220ef696",
   "execution_count": 302,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T01:10:51.696050Z",
     "start_time": "2024-09-24T01:08:47.774649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''Alteramos a lógica anterior para que a sequencia passada obrigatoriamente contenha palavras inteiras completas, e não corte palavras no meio. Isso foi feito alterando omínimo de código possível, só alteramos os indices de inicio e fim de uma sequência '''\n",
    "perplex = []\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    total_loss = 0\n",
    "    perplexidade_total = 0\n",
    "    count = 0\n",
    "    start_idx = 0\n",
    "    end_idx = max_sequence_len\n",
    "    \n",
    "    while start_idx < len(test_sentence) - max_sequence_len:\n",
    "        \n",
    "        while test_sentence[start_idx] != vocab[' ']:\n",
    "            start_idx += 1\n",
    "\n",
    "        while test_sentence[end_idx] != vocab[' '] and end_idx < len(test_sentence) - max_sequence_len:\n",
    "            end_idx -= 1\n",
    "            \n",
    "        start_idx += 1 # Pegar o primeiro token após o espaço\n",
    "        end_idx -= 1\n",
    "        \n",
    "        if start_idx >= end_idx:\n",
    "            '''Sequência com única palavra'''\n",
    "            continue\n",
    "\n",
    "        \n",
    "        input_tensor, target_tensor = create_input(test_sentence, start_idx, end_idx - start_idx) \n",
    "        loss = train(input_tensor, target_tensor)\n",
    "        total_loss += loss\n",
    "        \n",
    "        \n",
    "        perplexidade_total += perplexidade(rnn, shakespeare_data[start_idx: start_idx + max_sequence_len])\n",
    "        count +=1\n",
    "        \n",
    "        start_idx = end_idx\n",
    "        end_idx += max_sequence_len \n",
    "\n",
    "        \n",
    "    if epoch % print_every == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {total_loss}')\n",
    "        print(f'Perplexidade média nos dados de teste: {(perplexidade_total / (count)):.4f}')\n",
    "        perplex.append(perplexidade_total / count)\n",
    "        generate_rnn(rnn, prompt='when', tamanho_resposta=100)\n",
    "        print('\\n')\n",
    "\n",
    "models_perplexidade['RNN alteração 3'] = perplex"
   ],
   "id": "9789657ce893639d",
   "execution_count": 303,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Alteração 4** Adicionamos um schedular da learning_rate, de modo que ela decresce de acordo com o tempo (a cada época, conforme setado), além disso, alteramos a lógica do hidden no treinamento, fazendo o seguinte: Ao invés de a cada sequência iniciarmos o hidden zerado, iniciamos como o hidden da sequencia anterior, e cortamos o caminho do gradiente, para evitar uma dependencia de longo prazo. Além disso, ainda na ideia dos problemas de estabilidade das RNNS vanilla, adicionou-se a layer normalization, onde normalizamos as ativações de cada neurônio dentro de uma amostra específica, com isso, garantindo que as ativações de cada camada estejam dentro de uma faixa controlada",
   "id": "5bfd430b4af530c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:15:43.177843Z",
     "start_time": "2024-09-24T02:15:43.169786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding_dim = 128\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, embedding_dim, dropout_prob=0.5):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embeddings = nn.Embedding(input_size, embedding_dim)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        index = torch.nonzero(input).squeeze()\n",
    "        index = index[-1]\n",
    "        index = torch.tensor(index.item())\n",
    "\n",
    "        embs = self.embeddings(index)\n",
    "        embs = embs.unsqueeze(0)\n",
    "\n",
    "        input_combined = torch.cat((embs, hidden), 1)\n",
    "        hidden = self.i2h(input_combined)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.layer_norm(hidden)\n",
    "        hidden = self.dropout(hidden)\n",
    "\n",
    "        output = self.i2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.i2h.weight) # Inicialização Xavier para as camadas lineares\n",
    "        nn.init.xavier_uniform_(self.i2o.weight)\n",
    "        nn.init.zeros_(self.i2h.bias)\n",
    "        nn.init.zeros_(self.i2o.bias)\n",
    "\n",
    "        nn.init.uniform_(self.embeddings.weight, -1.0, 1.0)"
   ],
   "id": "57c4a0b7cd0cac60",
   "execution_count": 342,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:15:43.307829Z",
     "start_time": "2024-09-24T02:15:43.302137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_hidden = 128\n",
    "embedding_dim = 128\n",
    "rnn = RNN(vocab_len, n_hidden, vocab_len, embedding_dim)\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_epochs = 5\n",
    "print_every = 1\n",
    "max_sequence_len = 25\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "rnn"
   ],
   "id": "7bfc0eeb62dfdc44",
   "execution_count": 343,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:15:44.351739Z",
     "start_time": "2024-09-24T02:15:44.349248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(input_tensor, target_tensor, hidden):\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    loss = 0\n",
    "    for i in range(input_tensor.size(0)):\n",
    "        output, hidden = rnn(input_tensor[i], hidden)\n",
    "        loss += criterion(output, target_tensor[i].unsqueeze(0))\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5) #clipping de gradiente para evitar o exploding gradient\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item() / input_tensor.size(0), hidden.detach() #Retirar o rastreamento do hidden para a próxima iteração"
   ],
   "id": "bb6e2d6877c46130",
   "execution_count": 344,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:15:44.944576Z",
     "start_time": "2024-09-24T02:15:44.942298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "step_size = 1  # a cada época, reduzir a learning rate\n",
    "gamma = 0.9\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ],
   "id": "71f901b70ca2a8b7",
   "execution_count": 345,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:19:57.588784Z",
     "start_time": "2024-09-24T02:15:45.596239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "perplex = []\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    hidden = rnn.initHidden()\n",
    "    total_loss = 0\n",
    "    perplexidade_total = 0\n",
    "    count = 0\n",
    "    for start_idx in tqdm(range(0, len(test_sentence) - max_sequence_len, fator_desconto*max_sequence_len)):\n",
    "        input_tensor, target_tensor = create_input(test_sentence, start_idx, max_sequence_len)\n",
    "        loss, hidden = train(input_tensor, target_tensor, hidden)\n",
    "        total_loss += loss\n",
    "        perplexidade_total += perplexidade(rnn, shakespeare_data[start_idx: start_idx + max_sequence_len])\n",
    "        count +=1\n",
    "\n",
    "        \n",
    "        \n",
    "    scheduler.step()\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {total_loss}')\n",
    "    print(f'Perplexidade nos dados de teste: {(perplexidade_total / count):.4f}')\n",
    "    perplex.append(perplexidade_total / count)\n",
    "    generate_rnn(rnn, prompt='when', tamanho_resposta=100)\n",
    "\n",
    "models_perplexidade['RNN alteração 4'] = perplex"
   ],
   "id": "fc2aa50f06dcdaa",
   "execution_count": 346,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Comparativo",
   "id": "ce6bcbafc3b06f32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:20:01.482939Z",
     "start_time": "2024-09-24T02:20:01.362019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Lista de estilos de linha e cores para variedade\n",
    "estilos = ['-', '--', '-.', ':']\n",
    "cores = ['blue', 'green', 'red', 'orange', 'purple']\n",
    "\n",
    "for i, (label, valores) in enumerate(models_perplexidade.items()):\n",
    "    x = list(range(1, len(valores) + 1))\n",
    "    plt.plot(x, valores,\n",
    "             marker='o',\n",
    "             linestyle=estilos[i % len(estilos)],\n",
    "             color=cores[i % len(cores)],\n",
    "             label=label)\n",
    "\n",
    "plt.title('Perplexidade ao longo do Tempo')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Perplexidade')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "20e616e3e162b571",
   "execution_count": 347,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Com isso, percebemos que as alterações melhoraram quando avaliado a perplexidade em cima de dados nunca vistos pelo modelo. A não ser pela alteração 3, que inclui uma sequencia de treinamento com palavras completas. Na alteração 3, como o tamanho de sequência varia, isso pode ter interferido diretamente na perplexidade.",
   "id": "23961bb7de08d70c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Desafio",
   "id": "f6940a27d14a13e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As Redes Neurais Recorrentes (RNNs) são projetadas para trabalhar com dados sequenciais, onde cada saída depende das entradas anteriores e do estado oculto anterior. Apesar de, em teoria, conseguirem capturar dependências de longo prazo, na prática elas enfrentam problemas de desaparecimento ou explosão de gradientes durante o treinamento. Isso dificulta o aprendizado em sequências longas.\n",
    "\n",
    "Para superar esses desafios, surgiram as LSTMs (Long Short-Term Memory Networks) . Eles introduzem um mecanismo de memória mais inteligente que ajuda a preservar informações importantes ao longo do tempo. Aqui vai um resumo de como elas funcionam:\n",
    "\n",
    "Estado de Célula : Pensa nele como uma \"esteira\" que carrega informações relevantes ao longo do tempo, permitindo que dados importantes fluam sem muitas interferências.\n",
    "\n",
    "Três portas principais que controlam o fluxo de informações:\n",
    "\n",
    "Porta de Esquecimento : Decida se o estado da célula anterior deve ser esquecido. Ela analisa o estado oculto anterior e a entrada atual para determinar isso.\n",
    "Porta de Entrada : Determina quais novas informações serão selecionadas ao estado da célula. Ela cria um vetor de possíveis novas informações e decide o que realmente vai entrar.\n",
    "Porta de Saída ( output gate ): Decida qual estado da célula atual será usado para produzir a saída naquele momento.\n",
    "Essas portas usam funções de ativação específicas:\n",
    "\n",
    "Função Sigmoide : Usada nas portas para gerar valores entre 0 e 1, ajudando a controlar de forma proporcional o que deve ser desligado ou esquecido.\n",
    "Função Tangente Hiperbólica : Utilizada para criar o vetor de novas informações que podem ser adicionadas ao estado da célula, mantendo os valores entre -1 e 1.\n",
    "Analogias com Ligações Residuais :\n",
    "\n",
    "As portas nas LSTMs são semelhantes às conexões residuais em redes neurais profundas (como as ResNets). Ambas permitem que informações \"saltem\" etapas ou camadas, facilitando o fluxo de dados importantes e ajudando a evitar problemas com gradientes durante o treinamento.\n"
   ],
   "id": "547346a59fa4d68c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:42:39.853637Z",
     "start_time": "2024-09-24T02:42:39.844140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''Como temos um modelo word-level, nosso tokenizador deve quebrar o texto em palavras, e não em caracteres, assim, a entrada do modelo será uma sequência de palavras, e não de caracteres. Além disso, a saída do modelo será uma palavra, e não um caracter, assim, a saída do modelo será uma distribuição de probabilidade sobre o vocabulário, onde cada palavra terá uma probabilidade associada, e a palavra com maior probabilidade será a palavra prevista.'''\n",
    "\n",
    "\n",
    "def tokenize_data(corpus):\n",
    "    palavras = corpus.split()  # Quebra o texto em palavras\n",
    "\n",
    "    vocabulario = {palavra: i for i, palavra in enumerate(set(palavras))}\n",
    "    tokens = [vocabulario[palavra] for palavra in palavras]\n",
    "\n",
    "    return tokens, vocabulario\n",
    "\n",
    "\n",
    "'''Como vou criar o código da LSTM serão utilizados embeddings para representar uma palavra, com isso, a função create_input será alterada para retornar tensores com a representação númerica do token.'''\n",
    "\n",
    "def create_input(tokens, start_idx, max_sequence_len):\n",
    "    input = torch.tensor(tokens[start_idx: start_idx + max_sequence_len], dtype=torch.long) # Cria o tensor de entrada com tokens a partir do índice inicial, com comprimento máximo de sequência\n",
    "\n",
    "    target = torch.tensor(tokens[start_idx + 1: start_idx + max_sequence_len + 1], dtype=torch.long) # Cria o tensor de target que é o próximo token em relação à sequência de entrada\n",
    "\n",
    "    return input, target\n",
    "\n",
    "'''FUNÇÃO PREPROCESS_DATA'''\n",
    "def preprocess_data(corpus):\n",
    "    corpus = unicodedata.normalize('NFD', corpus)  # Remove acentuação\n",
    "    corpus = re.sub(r'\\s+', ' ', corpus)  # Substitui quebras de linha e espaços múltiplos por um único espaço\n",
    "    corpus = corpus.lower()  # Converte para minúsculas\n",
    "    corpus = re.sub(r'[^a-zA-Z0-9.,;:!?\\'\" -]', '', corpus)  # Remove tudo exceto letras, números, pontuações e espaços\n",
    "\n",
    "    #corpus = corpus.replace(';', '')\n",
    "    corpus = corpus.replace('--', '')\n",
    "    #corpus = corpus.replace(\"'\", '')\n",
    "    return corpus"
   ],
   "id": "ee6daf2c106af9de",
   "execution_count": 429,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:42:40.665Z",
     "start_time": "2024-09-24T02:42:40.066191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_sentence = load_dataset('obama-data.txt')\n",
    "\n",
    "test_sentence = preprocess_data(test_sentence)\n",
    "\n",
    "test_sentence = augment_text_by_word_freq(test_sentence, contagem_palavras, prob_map, stopword_prob=0.98)\n",
    "\n",
    "test_sentence, vocab = tokenize_data(test_sentence)\n",
    "\n",
    "vocab_len = len(vocab)\n",
    "vocab_ix_to_word = {v: k for k, v in vocab.items()}"
   ],
   "id": "555a47aefc8f35d7",
   "execution_count": 430,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:45:07.229457Z",
     "start_time": "2024-09-24T02:45:07.215983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, embedding_dim, dropout_prob=0.3):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embeddings = nn.Embedding(input_size, embedding_dim) # Camada de embeddings que representarão cada token/palavra\n",
    "\n",
    "        self.input_gate = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.forget_gate = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.output_gate = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.cell_candidate = nn.Linear(embedding_dim + hidden_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "\n",
    "        # Camada de saída\n",
    "        self.hidden2output = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, cell_state):\n",
    "        input = input.view(1)\n",
    "        embedded = self.embeddings(input)\n",
    "\n",
    "        combined = torch.cat((embedded, hidden), 1)\n",
    "\n",
    "        # Calcular as portas\n",
    "        i_t = torch.sigmoid(self.input_gate(combined))       # Porta de entrada\n",
    "        f_t = torch.sigmoid(self.forget_gate(combined))      # Porta de esquecimento\n",
    "        o_t = torch.sigmoid(self.output_gate(combined))      # Porta de saída\n",
    "        g_t = torch.tanh(self.cell_candidate(combined))      # Candidato a novo estado de célula\n",
    "\n",
    "        cell_state = f_t * cell_state + i_t * g_t # Atualizar o estado da célula\n",
    "\n",
    "        hidden = o_t * torch.tanh(cell_state) # Atualizar o estado oculto\n",
    "        \n",
    "        hidden = self.linear(hidden)\n",
    "    \n",
    "        hidden = F.tanh(hidden)\n",
    "        \n",
    "        output = self.hidden2output(hidden)\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output, hidden, cell_state\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "    def initCellState(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ],
   "id": "49604406f2c8fabc",
   "execution_count": 437,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:45:07.455101Z",
     "start_time": "2024-09-24T02:45:07.432445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_size = vocab_len\n",
    "hidden_size = 64\n",
    "output_size = vocab_len\n",
    "embedding_dim = 32\n",
    "n_epochs = 5\n",
    "print_every = 1\n",
    "max_sequence_len = 35\n",
    "learning_rate=0.0005\n",
    "lstm = LSTM(input_size, hidden_size, output_size, embedding_dim)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(lstm.parameters(), lr=learning_rate)"
   ],
   "id": "b148b7dc95cc25f",
   "execution_count": 438,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:45:07.625695Z",
     "start_time": "2024-09-24T02:45:07.622078Z"
    }
   },
   "cell_type": "code",
   "source": "lstm",
   "id": "65ad38df22fd9a8",
   "execution_count": 439,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:45:07.799880Z",
     "start_time": "2024-09-24T02:45:07.795878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_lstm(lstm, prompt=' ', tamanho_resposta=100):\n",
    "    lstm.eval()\n",
    "    oculto = lstm.initHidden()\n",
    "    estado_celula = lstm.initCellState()\n",
    "    prompt_processado = preprocess_data(prompt).split() \n",
    "\n",
    "\n",
    "    prompt_indices = []\n",
    "    for word in prompt_processado:\n",
    "        prompt_indices.append(vocab[word])\n",
    "\n",
    "    prompt_tensor = torch.tensor(prompt_indices, dtype=torch.long)\n",
    "\n",
    "    for token in prompt_tensor:\n",
    "        token_input = token.view(1)\n",
    "        output, oculto, estado_celula = lstm(token_input, oculto, estado_celula)\n",
    "\n",
    "    y_pred = output\n",
    "\n",
    "    # Geração do texto de resposta\n",
    "    for _ in range(tamanho_resposta):\n",
    "        token_previsto = torch.argmax(y_pred, dim=1).item()\n",
    "        print(vocab_ix_to_word[token_previsto], end=' ')\n",
    "        token_input = torch.tensor([token_previsto], dtype=torch.long)\n",
    "        y_pred, oculto, estado_celula = lstm(token_input, oculto, estado_celula)\n"
   ],
   "id": "f9fbd0197b308277",
   "execution_count": 440,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:45:07.993860Z",
     "start_time": "2024-09-24T02:45:07.991029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(input_tensor, target_tensor):\n",
    "    hidden = lstm.initHidden().detach()\n",
    "    cell_state = lstm.initCellState().detach()\n",
    "    lstm.zero_grad()\n",
    "    loss = 0\n",
    "    for i in range(input_tensor.size(0)):\n",
    "        output, hidden, cell_state = lstm(input_tensor[i], hidden, cell_state)\n",
    "        loss += criterion(output, target_tensor[i].unsqueeze(0))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item() / input_tensor.size(0)"
   ],
   "id": "9f8e6ae00fe3e0ae",
   "execution_count": 441,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:46:50.126038Z",
     "start_time": "2024-09-24T02:45:08.437873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    total_loss = 0\n",
    "    for start_idx in tqdm(range(0, len(test_sentence) - max_sequence_len, 50*max_sequence_len)):\n",
    "        input_tensor, target_tensor = create_input(test_sentence, start_idx, max_sequence_len)\n",
    "        loss = train(input_tensor, target_tensor)\n",
    "        total_loss += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print(f'Epoch: {epoch}, Loss: {total_loss}')\n",
    "        generate_lstm(lstm, 'badly', 30)"
   ],
   "id": "58a9ecfde0f00bc0",
   "execution_count": 442,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T02:40:29.011561Z",
     "start_time": "2024-09-24T02:40:29.011460Z"
    }
   },
   "cell_type": "code",
   "source": "generate_lstm(lstm, 'separatists', 30)",
   "id": "c37c13eaf21912f4",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
